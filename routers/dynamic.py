import os
import asyncio
from typing import Optional, Dict, Any, List
from fastapi import APIRouter, HTTPException, Query, Response
from fastapi.responses import StreamingResponse
from loguru import logger
import aiohttp
import aiofiles

from scripts.utils import load_config, setup_logger, get_output_path
from scripts.dynamic_db import (
    get_connection,
    save_normalized_dynamic_item,
    list_hosts_with_stats,
    list_dynamics_for_host,
    dynamic_core_exists,
)
from scripts.dynamic_media import collect_image_urls, download_images, predict_image_path, collect_live_media_urls, download_live_media, collect_emoji_urls, download_emojis

# ç¡®ä¿æ—¥å¿—ç³»ç»Ÿå·²åˆå§‹åŒ–
setup_logger()

router = APIRouter()

# ä»»åŠ¡ç®¡ç†ï¼šæŒ‰ host_mid ç®¡ç†æŠ“å–ä»»åŠ¡ã€åœæ­¢ä¿¡å·ä¸è¿›åº¦
_tasks = {}
_stop_events = {}
_progress = {}

def _get_or_create_event(host_mid: int) -> asyncio.Event:
    if host_mid not in _stop_events:
        _stop_events[host_mid] = asyncio.Event()
    return _stop_events[host_mid]

def _clear_event(host_mid: int) -> None:
    """æ¸…é™¤åœæ­¢äº‹ä»¶"""
    if host_mid in _stop_events:
        try:
            _stop_events[host_mid].clear()
            logger.info(f"[DEBUG] å·²æ¸…é™¤åœæ­¢äº‹ä»¶ host_mid={host_mid}")
        except Exception as e:
            logger.warning(f"[DEBUG] æ¸…é™¤åœæ­¢äº‹ä»¶å¤±è´¥: {e}")
    else:
        logger.info(f"[DEBUG] åœæ­¢äº‹ä»¶ä¸å­˜åœ¨äºç¼“å­˜ä¸­ host_mid={host_mid}")

def _set_progress(host_mid: int, page: int, total_items: int, last_offset: str, message: str) -> None:
    _progress[host_mid] = {
        "page": page,
        "total_items": total_items,
        "last_offset": last_offset or "",
        "message": message,
    }

def _get_progress(host_mid: int) -> Dict[str, Any]:
    return _progress.get(host_mid, {"page": 0, "total_items": 0, "last_offset": "", "message": "ç©ºé—²çŠ¶æ€ï¼Œæœªå¼€å§‹æŠ“å–"})


@router.get("/space/auto/{host_mid}/progress", summary="SSE å®æ—¶è·å–è‡ªåŠ¨æŠ“å–è¿›åº¦")
async def auto_fetch_progress(host_mid: int):
    """
    ä»¥ SSE æµæ–¹å¼æ¯ç§’æ¨é€ä¸€æ¬¡å½“å‰æŠ“å–è¿›åº¦ã€‚
    æ•°æ®æ ¼å¼ä¸º text/event-streamï¼Œdata ä¸º JSON å­—ç¬¦ä¸²ã€‚
    """
    async def event_generator():
        while True:
            progress = _get_progress(host_mid)
            # æ„é€  SSE åŒ…
            import json
            payload = json.dumps({
                "host_mid": host_mid,
                "page": progress.get("page", 0),
                "total_items": progress.get("total_items", 0),
                "last_offset": progress.get("last_offset", ""),
                "message": progress.get("message", "idle"),
            }, ensure_ascii=False)
            yield f"event: progress\ndata: {payload}\n\n"
            await asyncio.sleep(1)

    return StreamingResponse(event_generator(), media_type="text/event-stream")


@router.post("/space/auto/{host_mid}/stop", summary="åœæ­¢å½“å‰è‡ªåŠ¨æŠ“å–ï¼ˆé¡µçº§åœæ­¢ï¼‰")
async def stop_auto_fetch(host_mid: int):
    """
    å‘é€åœæ­¢ä¿¡å·ï¼Œå½“å‰é¡µå®Œæˆååœæ­¢æŠ“å–ï¼Œå¹¶è®°å½• offset ä»¥ä¾¿ä¸‹æ¬¡ç»§ç»­ã€‚
    æ³¨æ„ï¼šä¸‹æ¬¡å¼€å§‹æŠ“å–æ—¶ä¼šè‡ªåŠ¨æ¸…é™¤åœæ­¢ä¿¡å·ã€‚
    """
    ev = _get_or_create_event(host_mid)
    ev.set()
    logger.info(f"[DEBUG] å‘é€åœæ­¢ä¿¡å· host_mid={host_mid}, äº‹ä»¶çŠ¶æ€: is_set={ev.is_set()}")
    return {"status": "ok", "message": "stop signal sent", "event_is_set": ev.is_set()}


@router.get("/db/hosts", summary="åˆ—å‡ºæ•°æ®åº“ä¸­å·²æœ‰åŠ¨æ€çš„UPåˆ—è¡¨")
async def list_db_hosts(
    limit: int = Query(50, ge=1, le=200, description="æ¯é¡µæ•°é‡"),
    offset: int = Query(0, ge=0, description="åç§»é‡"),
):
    try:
        conn = get_connection()
    except Exception as e:
        logger.error(f"æ‰“å¼€åŠ¨æ€æ•°æ®åº“å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰“å¼€åŠ¨æ€æ•°æ®åº“å¤±è´¥: {str(e)}")
    try:
        data = list_hosts_with_stats(conn, limit=limit, offset=offset)

        # åŸºç¡€è¾“å‡ºæ ¹ç›®å½•ï¼ˆç”¨äºæ‹¼æ¥ç›¸å¯¹è·¯å¾„ï¼‰
        base_output_dir = os.path.dirname(get_output_path("__base__"))

        # ä¸ºæ¯ä¸ª host_mid å¢è¡¥ up_name ä¸ face_pathï¼ˆè‹¥å­˜åœ¨åˆ™è¿”å›ç›¸å¯¹è·¯å¾„ï¼‰
        cursor = conn.cursor()
        for rec in data:
            host_mid = rec.get("host_mid")
            up_name = None
            face_rel = None
            try:
                # æŸ¥è¯¢æœ€è¿‘ä¸€æ¡è®°å½•çš„ä½œè€…å
                row = cursor.execute(
                    """
                    SELECT author_name
                    FROM dynamic_core
                    WHERE host_mid = ? AND author_name IS NOT NULL AND author_name <> ''
                    ORDER BY (publish_ts IS NULL) ASC, publish_ts DESC, fetch_time DESC
                    LIMIT 1
                    """,
                    (str(host_mid),),
                ).fetchone()
                if row and row[0]:
                    up_name = row[0]
            except Exception as e:
                logger.warning(f"æŸ¥è¯¢ä½œè€…åå¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ host_mid={host_mid}: {e}")

            # å¤´åƒï¼šoutput/dynamic/{mid}/face.*
            try:
                host_dir = os.path.dirname(get_output_path("dynamic", str(host_mid), "__host_meta.json"))
                if os.path.isdir(host_dir):
                    for name in os.listdir(host_dir):
                        if os.path.isfile(os.path.join(host_dir, name)) and name.lower().startswith("face."):
                            face_rel = os.path.relpath(os.path.join(host_dir, name), base_output_dir)
                            break
            except Exception as e:
                logger.warning(f"å®šä½å¤´åƒå¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ host_mid={host_mid}: {e}")

        	# å†™å›æ‰©å±•å­—æ®µ
            rec["up_name"] = up_name
            rec["face_path"] = face_rel

        return {"data": data, "limit": limit, "offset": offset}
    finally:
        try:
            conn.close()
        except Exception:
            pass


@router.get("/db/space/{host_mid}", summary="åˆ—å‡ºæŒ‡å®šUPçš„åŠ¨æ€ï¼ˆæ¥è‡ªæ•°æ®åº“ï¼‰")
async def list_db_space(
    host_mid: int,
    limit: int = Query(20, ge=1, le=200, description="æ¯é¡µæ•°é‡"),
    offset: int = Query(0, ge=0, description="åç§»é‡"),
):
    try:
        conn = get_connection()
    except Exception as e:
        logger.error(f"æ‰“å¼€åŠ¨æ€æ•°æ®åº“å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰“å¼€åŠ¨æ€æ•°æ®åº“å¤±è´¥: {str(e)}")
    try:
        result = list_dynamics_for_host(conn, host_mid=host_mid, limit=limit, offset=offset)

        # å°† media_locals å’Œ live_media_locals ä»é€—å·åˆ†éš”å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°ç»„ï¼Œä¾¿äºå‰ç«¯ä½¿ç”¨
        try:
            items = result.get("items", []) if isinstance(result, dict) else []
            for item in items:
                # å¤„ç†æ™®é€šåª’ä½“
                ml = item.get("media_locals")
                if isinstance(ml, str):
                    ml_str = ml.strip()
                    if ml_str:
                        item["media_locals"] = [p for p in (s.strip() for s in ml_str.split(",")) if p]
                    else:
                        item["media_locals"] = []
                elif ml is None:
                    item["media_locals"] = []
                
                # å¤„ç†å®å†µåª’ä½“
                lml = item.get("live_media_locals")
                if isinstance(lml, str):
                    lml_str = lml.strip()
                    if lml_str:
                        item["live_media_locals"] = [p for p in (s.strip() for s in lml_str.split(",")) if p]
                    else:
                        item["live_media_locals"] = []
                elif lml is None:
                    item["live_media_locals"] = []
        except Exception as e:
            logger.warning(f"åª’ä½“è·¯å¾„è½¬æ¢å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ host_mid={host_mid}: {e}")

        return {"host_mid": str(host_mid), **result, "limit": limit, "offset": offset}
    finally:
        try:
            conn.close()
        except Exception:
            pass


def get_headers() -> Dict[str, str]:
    """è·å–è¯·æ±‚å¤´"""
    config = load_config()
    sessdata = config.get("SESSDATA", "")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36',
        'Referer': 'https://www.bilibili.com/',
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    }
    
    if sessdata:
        headers['Cookie'] = f'SESSDATA={sessdata}'
    
    return headers


async def fetch_dynamic_data(api_url: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """è·å–åŠ¨æ€æ•°æ®çš„é€šç”¨å‡½æ•°"""
    headers = get_headers()
    
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(api_url, headers=headers, params=params) as response:
                if response.status == 200:
                    # å†…å®¹ç±»å‹ä¿æŠ¤ï¼šä»…å½“è¿”å›ä¸º JSON æ—¶æ‰è§£æä¸º JSON
                    content_type = response.headers.get("Content-Type", "")
                    if "application/json" in content_type or "text/json" in content_type or "application/vnd" in content_type:
                        data = await response.json()
                    else:
                        # éJSONè¿”å›ï¼Œè¯»å–å°‘é‡æ–‡æœ¬ç”¨äºé”™è¯¯æç¤ºï¼ˆä¸æŠ›å‡ºäºŒæ¬¡å¼‚å¸¸ï¼‰
                        try:
                            snippet = (await response.text())[:256]
                        except Exception:
                            snippet = "<non-text response>"
                        logger.error(f"è¯·æ±‚è¿”å›éJSONï¼ŒContent-Type={content_type} url={api_url} params={params} snippet={snippet}")
                        raise HTTPException(status_code=500, detail="éJSONå“åº”ï¼Œæ— æ³•è§£æ")
                    logger.info(f"æˆåŠŸè·å–åŠ¨æ€æ•°æ®ï¼ŒçŠ¶æ€ç : {response.status}")
                    return data
                else:
                    logger.error(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                    raise HTTPException(status_code=response.status, detail=f"è¯·æ±‚å¤±è´¥: {response.status}")
        except aiohttp.ClientError as e:
            logger.error(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
            raise HTTPException(status_code=500, detail=f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {str(e)}")


@router.get("/space/auto/{host_mid}", summary="è‡ªåŠ¨ä»å‰åˆ°åæŠ“å–ç›´è‡³å®Œæˆ")
async def auto_fetch_all(
    host_mid: int,
    need_top: bool = Query(False, description="æ˜¯å¦éœ€è¦ç½®é¡¶åŠ¨æ€"),
    save_to_db: bool = Query(True, description="æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“"),
    save_media: bool = Query(True, description="æ˜¯å¦ä¿å­˜å›¾ç‰‡ç­‰å¤šåª’ä½“"),
):
    """
    è‡ªåŠ¨è¿ç»­æŠ“å–ç”¨æˆ·ç©ºé—´åŠ¨æ€ï¼š
    - ä»ä¸Šæ¬¡è®°å½•çš„offsetç»§ç»­ï¼›è‹¥å­˜åœ¨ fully_fetched=true åˆ™ä»å¤´å¼€å§‹
    - æ¯é¡µ3-5ç§’éšæœºå»¶è¿Ÿ
    - å½“ offset ä¸ºç©ºæ—¶ç»ˆæ­¢ï¼Œå†™ fully_fetched=true
    - è‹¥ä»å¤´å¼€å§‹æŠ“å–é‡åˆ°è¿ç»­10æ¡å·²å­˜åœ¨çš„åŠ¨æ€IDåˆ™åœæ­¢ï¼Œå¹¶ä¸ä¿å­˜è¿™10æ¡
    """
    import json, time, random

    api_url = "https://api.bilibili.com/x/polymer/web-dynamic/v1/feed/space"
    params = {
        "host_mid": host_mid, 
        "need_top": 1 if need_top else 0,
        "features": "itemOpusStyle,listOnlyfans,opusBigCover,onlyfansVote,forwardListHidden,decorationCard,commentsNewVersion,onlyfansAssetsV2,ugcDelete,onlyfansQaCard"
    }

    # è¯»å– host å…ƒæ•°æ®
    host_dir = os.path.dirname(get_output_path("dynamic", str(host_mid), "__host_meta.json"))
    os.makedirs(host_dir, exist_ok=True)
    meta_path = os.path.join(host_dir, "__host_meta.json")
    meta = {"host_mid": str(host_mid), "last_fetch_time": 0, "last_offset": {"offset": "", "update_baseline": "", "update_num": 0}, "fully_fetched": False}
    if os.path.exists(meta_path):
        try:
            with open(meta_path, "r", encoding="utf-8") as rf:
                old = json.load(rf)
            if isinstance(old, dict):
                meta.update(old)
        except Exception:
            pass

    # ç¡®å®šèµ·ç‚¹
    start_from_head = bool(meta.get("fully_fetched", False))
    next_offset = None if start_from_head else (meta.get("last_offset", {}) or {}).get("offset") or None
    
    # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°offsetä½¿ç”¨æƒ…å†µ
    logger.info(f"[DEBUG] æŠ“å–å¼€å§‹ host_mid={host_mid}")
    logger.info(f"[DEBUG] å…ƒæ•°æ®å†…å®¹: {meta}")
    logger.info(f"[DEBUG] fully_fetched={meta.get('fully_fetched')}, start_from_head={start_from_head}")
    logger.info(f"[DEBUG] last_offsetå¯¹è±¡: {meta.get('last_offset', {})}")
    logger.info(f"[DEBUG] å°†ä½¿ç”¨çš„next_offset: {next_offset}")
    if start_from_head:
        logger.info(f"[DEBUG] æ¨¡å¼: ä»å¤´å¼€å§‹æŠ“å– (å› ä¸ºfully_fetched=True)")
    elif next_offset:
        logger.info(f"[DEBUG] æ¨¡å¼: ä»offsetç»§ç»­æŠ“å– (offset={next_offset})")
    else:
        logger.info(f"[DEBUG] æ¨¡å¼: ä»å¤´å¼€å§‹æŠ“å– (æ— æœ‰æ•ˆoffset)")

    # DB è¿æ¥
    if save_to_db:
        try:
            conn = get_connection()
        except Exception as e:
            logger.error(f"æ‰“å¼€åŠ¨æ€æ•°æ®åº“å¤±è´¥: {e}")
            raise HTTPException(status_code=500, detail=f"æ‰“å¼€åŠ¨æ€æ•°æ®åº“å¤±è´¥: {str(e)}")
    else:
        conn = None

    all_items = []
    consecutive_duplicates = 0

    # é¡µè®¡æ•°
    current_page = 0
    
    # è‡ªåŠ¨é‡ç½®åœæ­¢ä¿¡å·ï¼šæ¯æ¬¡å¼€å§‹æŠ“å–å‰éƒ½æ¸…é™¤ä¹‹å‰çš„åœæ­¢çŠ¶æ€
    logger.info(f"[DEBUG] è‡ªåŠ¨é‡ç½®åœæ­¢ä¿¡å· host_mid={host_mid}")
    _clear_event(host_mid)  # ç¡®ä¿æ¸…é™¤ä»»ä½•é—ç•™çš„åœæ­¢ä¿¡å·
    stop_event = _get_or_create_event(host_mid)  # åˆ›å»ºæ–°çš„äº‹ä»¶å¯¹è±¡
    
    # éªŒè¯é‡ç½®ç»“æœ
    logger.info(f"[DEBUG] é‡ç½®ååœæ­¢äº‹ä»¶çŠ¶æ€: is_set={stop_event.is_set()}")
    
    _set_progress(host_mid, current_page, 0, next_offset or "", "å‡†å¤‡å¼€å§‹æŠ“å–åŠ¨æ€")

    try:
        while True:
            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ¯é¡µè¯·æ±‚çš„å‚æ•°
            logger.info(f"[DEBUG] === ç¬¬ {current_page + 1} é¡µè¯·æ±‚ ===")
            logger.info(f"[DEBUG] å½“å‰next_offset: {next_offset}")
            
            if next_offset:
                params["offset"] = next_offset
                logger.info(f"[DEBUG] è®¾ç½®offsetå‚æ•°: {next_offset}")
            elif "offset" in params:
                params.pop("offset", None)
                logger.info(f"[DEBUG] ç§»é™¤offsetå‚æ•°ï¼ˆä»å¤´å¼€å§‹ï¼‰")
            
            logger.info(f"[DEBUG] æœ€ç»ˆè¯·æ±‚å‚æ•°: {params}")

            # æ›´æ–°è¿›åº¦ï¼šå‡†å¤‡æŠ“å–ä¸‹ä¸€é¡µ
            if current_page > 0:
                _set_progress(host_mid, current_page, len(all_items), next_offset or "", f"å‡†å¤‡æŠ“å–ç¬¬ {current_page + 1} é¡µ...")
            
            # éšæœºå»¶è¿Ÿ3-5ç§’
            await asyncio.sleep(random.uniform(3, 5))

            data = await fetch_dynamic_data(api_url, params)
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°APIå“åº”ä¸­çš„offsetä¿¡æ¯
            logger.info(f"[DEBUG] APIå“åº”ç»“æ„æ£€æŸ¥:")
            logger.info(f"[DEBUG] - response.code: {data.get('code')}")
            logger.info(f"[DEBUG] - response.dataå­˜åœ¨: {bool(data.get('data'))}")
            
            # å…¼å®¹Bç«™åŸå§‹ç»“æ„ï¼šä»data.dataä¸­è·å–itemså’Œoffset
            data_section = data.get("data", {}) if isinstance(data, dict) else {}
            items = data_section.get("items", []) if isinstance(data_section, dict) else []
            # offset æ—¢å¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼Œä¹Ÿå¯èƒ½åœ¨å¯¹è±¡ä¸­
            off = data_section.get("offset") if isinstance(data_section, dict) else None
            
            logger.info(f"[DEBUG] - data_section.offsetåŸå§‹å€¼: {off} (ç±»å‹: {type(off)})")
            
            if isinstance(off, dict):
                next_offset = off.get("offset")
                logger.info(f"[DEBUG] - ä»offsetå¯¹è±¡æå–: {next_offset}")
            else:
                next_offset = off
                logger.info(f"[DEBUG] - ç›´æ¥ä½¿ç”¨offset: {next_offset}")
            
            logger.info(f"[DEBUG] - æœ¬é¡µè·å–itemsæ•°é‡: {len(items)}")
            logger.info(f"[DEBUG] - ä¸‹ä¸€é¡µçš„offset: {next_offset}")

            # è‹¥ä»å¤´å¼€å§‹ï¼Œå¹¶ä¸”å‡ºç°è¿ç»­10æ¡éƒ½å·²å­˜åœ¨ï¼Œåˆ™åœæ­¢
            if start_from_head and conn is not None:
                for item in items:
                    id_str = item.get("id_str") or item.get("basic", {}).get("id_str") or str(item.get("id"))
                    if id_str and dynamic_core_exists(conn, host_mid, str(id_str)):
                        consecutive_duplicates += 1
                        if consecutive_duplicates >= 10:
                            next_offset = None  # è§¦å‘ç»ˆæ­¢
                            items = []  # ä¸ä¿å­˜è¿™10æ¡
                            break
                    else:
                        consecutive_duplicates = 0

            all_items.extend(items)
            current_page += 1
            _set_progress(host_mid, current_page, len(all_items), next_offset or "", f"ç¬¬ {current_page} é¡µæŠ“å–å®Œæˆï¼Œæœ¬é¡µè·å– {len(items)} æ¡åŠ¨æ€ï¼Œç´¯è®¡ {len(all_items)} æ¡")

            # ä¿å­˜é¡µé¢æ•°æ®ï¼ˆå»æ‰item.jsonä¿å­˜ï¼Œåªæœ‰åŒ…å«å¤šåª’ä½“æ–‡ä»¶æ—¶æ‰åˆ›å»ºæ–‡ä»¶å¤¹ï¼‰
            if save_to_db and items:
                base_output_dir = os.path.dirname(get_output_path("__base__"))
                
                # å¤´åƒä¿å­˜ï¼šä»…ä¿å­˜ä¸€æ¬¡åˆ° output/dynamic/{host_mid}/face.(ext)
                try:
                    # å°è¯•ä»itemsæå–ç”¨æˆ·å¤´åƒ
                    face_url = None
                    for item in items:
                        modules_raw = item.get("modules")
                        if isinstance(modules_raw, dict):
                            face_url = modules_raw.get("module_author", {}).get("face")
                        elif isinstance(modules_raw, list):
                            for mod in modules_raw:
                                if isinstance(mod, dict) and mod.get("module_type") == "MODULE_TYPE_AUTHOR":
                                    face_url = mod.get("module_author", {}).get("user", {}).get("face") or mod.get("module_author", {}).get("face")
                                    if face_url:
                                        break
                        if not face_url:
                            face_url = item.get("user", {}).get("face")
                        if face_url:
                            break

                    if face_url:
                        # è‹¥å·²å­˜åœ¨å¤´åƒæ–‡ä»¶åˆ™è·³è¿‡
                        host_dir = os.path.dirname(get_output_path("dynamic", str(host_mid), "__host_meta.json"))
                        os.makedirs(host_dir, exist_ok=True)

                        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä»»ä½•æ–‡ä»¶ä»¥ face.* å‘½å
                        exists = any(
                            name.lower().startswith("face.")
                            for name in os.listdir(host_dir)
                            if os.path.isfile(os.path.join(host_dir, name))
                        )
                        if not exists:
                            # ä¸‹è½½å¤´åƒä¸€æ¬¡
                            results = await download_images([face_url], host_dir)
                            # å°†ä¸‹è½½çš„å“ˆå¸Œæ–‡ä»¶é‡å‘½åä¸º face.æ‰©å±•å
                            for url, local_path, ok in results:
                                if ok:
                                    _, ext = os.path.splitext(local_path)
                                    new_path = os.path.join(host_dir, f"face{ext}")
                                    try:
                                        if os.path.exists(new_path):
                                            os.remove(new_path)
                                    except Exception:
                                        pass
                                    try:
                                        os.replace(local_path, new_path)
                                    except Exception:
                                        pass
                                    break
                except Exception as e:
                    logger.warning(f"ä¿å­˜å¤´åƒå¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{e}")
                
                for item in items:
                    try:
                        id_str = (
                            item.get("id_str")
                            or item.get("basic", {}).get("id_str")
                            or str(item.get("id"))
                        )
                        if not id_str:
                            continue

                        # æ£€æŸ¥æ˜¯å¦æœ‰å¤šåª’ä½“æ–‡ä»¶éœ€è¦ä¸‹è½½
                        has_media = False
                        predicted_locals = []
                        live_predicted_locals = []
                        if save_media:
                            # å¤„ç†æ™®é€šå›¾ç‰‡
                            image_urls = collect_image_urls(item)
                            if image_urls:
                                has_media = True
                                # åªæœ‰å½“åŒ…å«å¤šåª’ä½“æ–‡ä»¶æ—¶æ‰åˆ›å»ºæ–‡ä»¶å¤¹
                                item_dir = os.path.dirname(
                                    get_output_path("dynamic", str(host_mid), str(id_str), "media")
                                )
                                os.makedirs(item_dir, exist_ok=True)
                                
                                # é¢„æµ‹æœ¬åœ°è·¯å¾„
                                for u in image_urls:
                                    predicted_locals.append(os.path.relpath(predict_image_path(u, item_dir), base_output_dir))
                                
                                results = await download_images(image_urls, item_dir)
                                media_records = []
                                for media_url, local_path, ok in results:
                                    if ok:
                                        rel_path = os.path.relpath(local_path, base_output_dir)
                                        media_records.append((media_url, rel_path, "image"))
                            
                            # å¤„ç†å®å†µåª’ä½“ï¼ˆliveå›¾ç‰‡+è§†é¢‘ï¼‰
                            live_media_pairs = collect_live_media_urls(item)
                            if live_media_pairs:
                                has_media = True
                                # åˆ›å»ºæ–‡ä»¶å¤¹ï¼ˆå¦‚æœè¿˜æœªåˆ›å»ºï¼‰
                                if not image_urls:
                                    item_dir = os.path.dirname(
                                        get_output_path("dynamic", str(host_mid), str(id_str), "media")
                                    )
                                    os.makedirs(item_dir, exist_ok=True)
                                
                                live_results = await download_live_media(live_media_pairs, item_dir)
                                for image_url, video_url, image_path, video_path, ok in live_results:
                                    if ok:
                                        # å°†å®å†µåª’ä½“è·¯å¾„åˆ†åˆ«è®°å½•
                                        image_rel = os.path.relpath(image_path, base_output_dir)
                                        video_rel = os.path.relpath(video_path, base_output_dir)
                                        live_predicted_locals.extend([image_rel, video_rel])
                            
                            # å¤„ç†è¡¨æƒ…
                            emoji_pairs = collect_emoji_urls(item)
                            if emoji_pairs:
                                has_media = True
                                # åˆ›å»ºæ–‡ä»¶å¤¹ï¼ˆå¦‚æœè¿˜æœªåˆ›å»ºï¼‰
                                if not image_urls and not live_media_pairs:
                                    item_dir = os.path.dirname(
                                        get_output_path("dynamic", str(host_mid), str(id_str), "media")
                                    )
                                    os.makedirs(item_dir, exist_ok=True)
                                
                                emoji_results = await download_emojis(emoji_pairs, item_dir)
                                for emoji_url, emoji_path, ok in emoji_results:
                                    if ok:
                                        # å°†è¡¨æƒ…è·¯å¾„è®°å½•åˆ°æ™®é€šåª’ä½“ä¸­
                                        emoji_rel = os.path.relpath(emoji_path, base_output_dir)
                                        predicted_locals.append(emoji_rel)

                        # è§„èŒƒåŒ–ä¿å­˜åˆ°æ•°æ®åº“
                        logger.info(f"normalize.core.call begin host_mid={host_mid} id_str={id_str}")
                        try:
                            save_normalized_dynamic_item(conn, host_mid, item)
                            logger.info(f"normalize.core.call done host_mid={host_mid} id_str={id_str}")
                            # å›å†™æœ¬åœ°è·¯å¾„é€—å·ä¸²ï¼ˆåªæœ‰å½“æœ‰å¤šåª’ä½“æ–‡ä»¶æ—¶ï¼‰
                            if predicted_locals or live_predicted_locals:
                                cursor = conn.cursor()
                                cursor.execute(
                                    """
                                    UPDATE dynamic_core SET
                                        media_locals = CASE
                                            WHEN media_locals IS NULL OR media_locals = '' THEN ?
                                            ELSE media_locals
                                        END,
                                        live_media_locals = CASE
                                            WHEN live_media_locals IS NULL OR live_media_locals = '' THEN ?
                                            ELSE live_media_locals
                                        END,
                                        live_media_count = ?
                                    WHERE host_mid = ? AND id_str = ?
                                    """,
                                    (
                                        ",".join(predicted_locals) if predicted_locals else "",
                                        ",".join(live_predicted_locals) if live_predicted_locals else "",
                                        len(live_predicted_locals),
                                        str(host_mid),
                                        str(id_str),
                                    ),
                                )
                                conn.commit()
                        except Exception as norm_err:
                            logger.warning(f"è§„èŒƒåŒ–ä¿å­˜å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰: {norm_err}")
                    except Exception as perr:
                        logger.warning(f"ä¿å­˜é¡µé¢æ•°æ®å¤±è´¥: {perr}")

            # æ›´æ–° meta
            meta["last_fetch_time"] = int(time.time())
            meta["last_offset"] = {"offset": next_offset or "", "update_baseline": "", "update_num": 0}
            meta["fully_fetched"] = not bool(next_offset)
            
            # è°ƒè¯•ä¿¡æ¯ï¼šä¿å­˜çŠ¶æ€
            logger.info(f"[DEBUG] ä¿å­˜å…ƒæ•°æ®:")
            logger.info(f"[DEBUG] - last_offset.offset: {next_offset or ''}")
            logger.info(f"[DEBUG] - fully_fetched: {meta['fully_fetched']}")
            
            try:
                async with aiofiles.open(meta_path, "w", encoding="utf-8") as wf:
                    await wf.write(json.dumps(meta, ensure_ascii=False, indent=2))
                logger.info(f"[DEBUG] å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {meta_path}")
            except Exception as e:
                logger.error(f"[DEBUG] ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {e}")

            # ç»ˆæ­¢æ¡ä»¶ï¼šoffset ä¸ºç©º
            logger.info(f"[DEBUG] æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶: next_offset={next_offset}, æ˜¯å¦ä¸ºç©º: {not bool(next_offset)}")
            if not next_offset:
                _set_progress(host_mid, current_page, len(all_items), next_offset or "", f"[å…¨éƒ¨æŠ“å–å®Œæ¯•] æŠ“å–å®Œæˆï¼å…±è·å– {len(all_items)} æ¡åŠ¨æ€ï¼Œæ€»è®¡ {current_page} é¡µ")
                break

            # é¡µçº§åœæ­¢ï¼šå¦‚æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œåˆ™æŠ“å–å®Œæœ¬é¡µååœæ­¢å¹¶è®°å½• offset
            logger.info(f"[DEBUG] æ£€æŸ¥åœæ­¢ä¿¡å·: is_set={stop_event.is_set()}")
            if stop_event.is_set():
                logger.warning(f"[DEBUG] ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œåœæ­¢æŠ“å–")
                _set_progress(host_mid, current_page, len(all_items), next_offset or "", f"ç”¨æˆ·åœæ­¢æŠ“å–ï¼Œå·²å®Œæˆ {current_page} é¡µï¼Œå…±è·å– {len(all_items)} æ¡åŠ¨æ€")
                break

        # è¿”å›Bç«™åŸå§‹ç»“æ„ï¼Œåˆå¹¶æ‰€æœ‰items
        return {
            "code": 0,
            "message": "0",
            "ttl": 1,
            "data": {
                "has_more": bool(next_offset),
                "items": all_items,
                "offset": next_offset or "",
                "update_baseline": "",
                "update_num": 0,
                "fully_fetched": meta.get("fully_fetched", False),
            }
        }
    finally:
        if conn is not None:
            try:
                conn.close()
            except Exception:
                pass
        # å…³é—­åä¿æŒæœ€åä¸€æ¬¡è¿›åº¦


@router.get("/space/{host_mid}", summary="è·å–ç”¨æˆ·ç©ºé—´åŠ¨æ€")
async def get_space_dynamic(
    host_mid: int,
    pages: int = Query(1, description="è·å–é¡µæ•°ï¼Œ0 è¡¨ç¤ºè·å–å…¨éƒ¨"),
    need_top: bool = Query(False, description="æ˜¯å¦éœ€è¦ç½®é¡¶åŠ¨æ€"),
    save_to_db: bool = Query(True, description="æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“"),
    save_media: bool = Query(True, description="æ˜¯å¦ä¿å­˜å›¾ç‰‡ç­‰å¤šåª’ä½“")
):
    """
    è·å–æŒ‡å®šç”¨æˆ·ç©ºé—´çš„åŠ¨æ€åˆ—è¡¨
    
    Args:
        host_mid: ç›®æ ‡ç”¨æˆ·çš„UID
        offset: åˆ†é¡µåç§»é‡
        need_top: æ˜¯å¦è·å–ç½®é¡¶åŠ¨æ€
    """
    try:
        api_url = "https://api.bilibili.com/x/polymer/web-dynamic/v1/feed/space"
        
        params = {
            "host_mid": host_mid,
            "need_top": 1 if need_top else 0,
            "features": "itemOpusStyle,listOnlyfans,opusBigCover,onlyfansVote,forwardListHidden,decorationCard,commentsNewVersion,onlyfansAssetsV2,ugcDelete,onlyfansQaCard"
        }
        
        logger.info(f"è¯·æ±‚ç”¨æˆ· {host_mid} çš„ç©ºé—´åŠ¨æ€ï¼Œå‚æ•°: {params}, pages={pages}")

        # å¤šé¡µæŠ“å–ï¼šè‡³å¤š pages é¡µï¼›è‹¥ pages=0 åˆ™ç›´åˆ° offset ä¸ºç©º
        all_items = []
        next_offset: Optional[str] = None
        current_page = 0
        while True:
            # æ³¨å…¥åç§»
            if next_offset:
                params["offset"] = next_offset
            
            data = await fetch_dynamic_data(api_url, params)
            # å…¼å®¹Bç«™åŸå§‹ç»“æ„ï¼šä»data.dataä¸­è·å–itemså’Œoffset
            data_section = data.get("data", {}) if isinstance(data, dict) else {}
            items = data_section.get("items", []) if isinstance(data_section, dict) else []
            off = data_section.get("offset") if isinstance(data_section, dict) else None
            if isinstance(off, dict):
                next_offset = off.get("offset")
            else:
                next_offset = off

            all_items.extend(items)
            current_page += 1

            # ç»ˆæ­¢æ¡ä»¶
            if pages == 0:
                if not next_offset:
                    break
            else:
                if current_page >= max(1, pages):
                    break
                if not next_offset:
                    break

        # å¯é€‰ä¿å­˜
        if save_to_db:
            try:
                conn = get_connection()
            except Exception as e:
                logger.error(f"æ‰“å¼€åŠ¨æ€æ•°æ®åº“å¤±è´¥: {e}")
                raise HTTPException(status_code=500, detail=f"æ‰“å¼€åŠ¨æ€æ•°æ®åº“å¤±è´¥: {str(e)}")

            items: List[Dict[str, Any]] = all_items

            base_output_dir = os.path.dirname(get_output_path("__base__"))

            # å¤´åƒä¿å­˜ï¼šä»…ä¿å­˜ä¸€æ¬¡åˆ° output/dynamic/{host_mid}/face.(ext)
            try:
                # å°è¯•ä»itemsæå–ç”¨æˆ·å¤´åƒ
                face_url = None
                for item in items:
                    modules_raw = item.get("modules")
                    if isinstance(modules_raw, dict):
                        face_url = modules_raw.get("module_author", {}).get("face")
                    elif isinstance(modules_raw, list):
                        for mod in modules_raw:
                            if isinstance(mod, dict) and mod.get("module_type") == "MODULE_TYPE_AUTHOR":
                                face_url = mod.get("module_author", {}).get("user", {}).get("face") or mod.get("module_author", {}).get("face")
                                if face_url:
                                    break
                    if not face_url:
                        face_url = item.get("user", {}).get("face")
                    if face_url:
                        break

                if face_url:
                    # è‹¥å·²å­˜åœ¨å¤´åƒæ–‡ä»¶åˆ™è·³è¿‡
                    host_dir = os.path.dirname(get_output_path("dynamic", str(host_mid), "__host_meta.json"))
                    os.makedirs(host_dir, exist_ok=True)

                    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä»»ä½•æ–‡ä»¶ä»¥ face.* å‘½å
                    exists = any(
                        name.lower().startswith("face.")
                        for name in os.listdir(host_dir)
                        if os.path.isfile(os.path.join(host_dir, name))
                    )
                    if not exists:
                        # ä¸‹è½½å¤´åƒä¸€æ¬¡
                        results = await download_images([face_url], host_dir)
                        # å°†ä¸‹è½½çš„å“ˆå¸Œæ–‡ä»¶é‡å‘½åä¸º face.æ‰©å±•å
                        for url, local_path, ok in results:
                            if ok:
                                _, ext = os.path.splitext(local_path)
                                new_path = os.path.join(host_dir, f"face{ext}")
                                try:
                                    if os.path.exists(new_path):
                                        os.remove(new_path)
                                except Exception:
                                    pass
                                try:
                                    os.replace(local_path, new_path)
                                except Exception:
                                    pass
                                break
            except Exception as e:
                logger.warning(f"ä¿å­˜å¤´åƒå¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{e}")

            # å†™ host_mid å…ƒæ•°æ®ï¼šæœ€åä¸€æ¬¡è·å–çš„æ—¶é—´ä¸offset
            try:
                import json, time
                host_dir = os.path.dirname(get_output_path("dynamic", str(host_mid), "__host_meta.json"))
                meta_path = os.path.join(host_dir, "__host_meta.json")

                last_offset_obj = {"offset": next_offset or "", "update_baseline": "", "update_num": 0}
                meta = {
                    "host_mid": str(host_mid),
                    "last_fetch_time": int(time.time()),
                    "last_offset": last_offset_obj,
                    "fully_fetched": False,
                }

                # åˆå¹¶æ—§å€¼ï¼ˆä¿ç•™ fully_fetched ç­‰çŠ¶æ€ï¼‰
                if os.path.exists(meta_path):
                    try:
                        with open(meta_path, "r", encoding="utf-8") as rf:
                            old = json.load(rf)
                        if isinstance(old, dict):
                            meta.update({k: old.get(k, meta.get(k)) for k in ("fully_fetched",)})
                    except Exception:
                        pass

                async with aiofiles.open(meta_path, "w", encoding="utf-8") as wf:
                    await wf.write(json.dumps(meta, ensure_ascii=False, indent=2))
            except Exception as e:
                logger.warning(f"å†™å…¥ host_mid å…ƒæ•°æ®å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{e}")

            for item in items:
                try:
                    id_str = (
                        item.get("id_str")
                        or item.get("basic", {}).get("id_str")
                        or str(item.get("id"))
                    )
                    if not id_str:
                        # è·³è¿‡æ— æ³•å®šä½IDçš„è®°å½•
                        continue

                    # æ£€æŸ¥æ˜¯å¦æœ‰å¤šåª’ä½“æ–‡ä»¶éœ€è¦ä¸‹è½½
                    has_media = False
                    predicted_locals = []
                    live_predicted_locals = []
                    if save_media:
                        # å¤„ç†æ™®é€šå›¾ç‰‡
                        image_urls = collect_image_urls(item)
                        if image_urls:
                            has_media = True
                            # åªæœ‰å½“åŒ…å«å¤šåª’ä½“æ–‡ä»¶æ—¶æ‰åˆ›å»ºæ–‡ä»¶å¤¹
                            item_dir = os.path.dirname(
                                get_output_path("dynamic", str(host_mid), str(id_str), "media")
                            )
                            os.makedirs(item_dir, exist_ok=True)
                            
                            # é¢„æµ‹æœ¬åœ°è·¯å¾„
                            for u in image_urls:
                                predicted_locals.append(os.path.relpath(predict_image_path(u, item_dir), base_output_dir))
                            
                            # ä¸‹è½½å¤šåª’ä½“æ–‡ä»¶
                            results = await download_images(image_urls, item_dir)
                            media_records = []
                            for media_url, local_path, ok in results:
                                if ok:
                                    rel_path = os.path.relpath(local_path, base_output_dir)
                                    media_records.append((media_url, rel_path, "image"))
                        
                        # å¤„ç†å®å†µåª’ä½“ï¼ˆliveå›¾ç‰‡+è§†é¢‘ï¼‰
                        live_media_pairs = collect_live_media_urls(item)
                        if live_media_pairs:
                            has_media = True
                            # åˆ›å»ºæ–‡ä»¶å¤¹ï¼ˆå¦‚æœè¿˜æœªåˆ›å»ºï¼‰
                            if not image_urls:
                                item_dir = os.path.dirname(
                                    get_output_path("dynamic", str(host_mid), str(id_str), "media")
                                )
                                os.makedirs(item_dir, exist_ok=True)
                            
                            live_results = await download_live_media(live_media_pairs, item_dir)
                            for image_url, video_url, image_path, video_path, ok in live_results:
                                if ok:
                                    # å°†å®å†µåª’ä½“è·¯å¾„åˆ†åˆ«è®°å½•
                                    image_rel = os.path.relpath(image_path, base_output_dir)
                                    video_rel = os.path.relpath(video_path, base_output_dir)
                                    live_predicted_locals.extend([image_rel, video_rel])
                        
                        # å¤„ç†è¡¨æƒ…
                        emoji_pairs = collect_emoji_urls(item)
                        if emoji_pairs:
                            has_media = True
                            # åˆ›å»ºæ–‡ä»¶å¤¹ï¼ˆå¦‚æœè¿˜æœªåˆ›å»ºï¼‰
                            if not image_urls and not live_media_pairs:
                                item_dir = os.path.dirname(
                                    get_output_path("dynamic", str(host_mid), str(id_str), "media")
                                )
                                os.makedirs(item_dir, exist_ok=True)
                            
                            emoji_results = await download_emojis(emoji_pairs, item_dir)
                            for emoji_url, emoji_path, ok in emoji_results:
                                if ok:
                                    # å°†è¡¨æƒ…è·¯å¾„è®°å½•åˆ°æ™®é€šåª’ä½“ä¸­
                                    emoji_rel = os.path.relpath(emoji_path, base_output_dir)
                                    predicted_locals.append(emoji_rel)

                    # è§„èŒƒåŒ–ä¿å­˜åˆ°æ•°æ®åº“
                    logger.info(f"normalize.core.call begin host_mid={host_mid} id_str={id_str}")
                    try:
                        save_normalized_dynamic_item(conn, host_mid, item)
                        logger.info(f"normalize.core.call done host_mid={host_mid} id_str={id_str}")
                        # å›å†™æœ¬åœ°è·¯å¾„é€—å·ä¸²ï¼ˆåªæœ‰å½“æœ‰å¤šåª’ä½“æ–‡ä»¶æ—¶ï¼‰
                        if predicted_locals or live_predicted_locals:
                            cursor = conn.cursor()
                            cursor.execute(
                                """
                                UPDATE dynamic_core SET
                                    media_locals = CASE
                                        WHEN media_locals IS NULL OR media_locals = '' THEN ?
                                        ELSE media_locals
                                    END,
                                    live_media_locals = CASE
                                        WHEN live_media_locals IS NULL OR live_media_locals = '' THEN ?
                                        ELSE live_media_locals
                                    END,
                                    live_media_count = ?
                                WHERE host_mid = ? AND id_str = ?
                                """,
                                (
                                    ",".join(predicted_locals) if predicted_locals else "",
                                    ",".join(live_predicted_locals) if live_predicted_locals else "",
                                    len(live_predicted_locals),
                                    str(host_mid),
                                    str(id_str),
                                ),
                            )
                            conn.commit()
                    except Exception as norm_err:
                        logger.warning(f"è§„èŒƒåŒ–ä¿å­˜å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰: {norm_err}")
                except Exception as perr:
                    logger.error(f"ä¿å­˜åŠ¨æ€é¡¹å¤±è´¥ id_str={item.get('id_str')}: {perr}")

            try:
                conn.close()
            except Exception:
                pass

        # è¿”å›Bç«™åŸå§‹ç»“æ„ï¼Œåˆå¹¶æ‰€æœ‰items
        return {
            "code": 0,
            "message": "0",
            "ttl": 1,
            "data": {
                "has_more": bool(next_offset),
                "items": all_items,
                "offset": next_offset or "",
                "update_baseline": "",
                "update_num": 0
            }
        }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–ç”¨æˆ·ç©ºé—´åŠ¨æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ç”¨æˆ·ç©ºé—´åŠ¨æ€å¤±è´¥: {str(e)}")


@router.get("/detail/{dynamic_id}", summary="è·å–åŠ¨æ€è¯¦æƒ…")
async def get_dynamic_detail(
    dynamic_id: str,
    save_to_db: bool = Query(True, description="æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“"),
    save_media: bool = Query(True, description="æ˜¯å¦ä¿å­˜å›¾ç‰‡ç­‰å¤šåª’ä½“")
):
    """
    è·å–å•æ¡åŠ¨æ€çš„è¯¦ç»†ä¿¡æ¯
    
    Args:
        dynamic_id: åŠ¨æ€ID
    """
    try:
        api_url = "https://api.bilibili.com/x/polymer/web-dynamic/v1/detail"
        
        params = {
            "id": dynamic_id
        }
        
        logger.info(f"è¯·æ±‚åŠ¨æ€è¯¦æƒ…ï¼ŒID: {dynamic_id}")
        
        data = await fetch_dynamic_data(api_url, params)
        
        if save_to_db:
            try:
                conn = get_connection()
            except Exception as e:
                logger.error(f"æ‰“å¼€åŠ¨æ€æ•°æ®åº“å¤±è´¥: {e}")
                raise HTTPException(status_code=500, detail=f"æ‰“å¼€åŠ¨æ€æ•°æ®åº“å¤±è´¥: {str(e)}")

            # detail æ¥å£é€šå¸¸è¿”å›ä¸€ä¸ª item
            data_section = data.get("data", {}) if isinstance(data, dict) else {}
            item = data_section.get("item") or data_section.get("card") or data_section

            if isinstance(item, dict):
                try:
                    id_str = (
                        item.get("id_str")
                        or item.get("basic", {}).get("id_str")
                        or str(item.get("id") or dynamic_id)
                    )

                    # å°è¯•ä»ä½œè€…ä¿¡æ¯è§£æ host_midï¼Œå¤±è´¥åˆ™ç”¨ 0
                    host_mid_val = (
                        item.get("modules", {})
                        .get("module_author", {})
                        .get("mid")
                    )
                    try:
                        host_mid_int = int(host_mid_val) if host_mid_val is not None else 0
                    except Exception:
                        host_mid_int = 0

                    # æ£€æŸ¥æ˜¯å¦æœ‰å¤šåª’ä½“æ–‡ä»¶éœ€è¦ä¸‹è½½
                    base_output_dir = os.path.dirname(get_output_path("__base__"))
                    has_media = False
                    predicted_locals = []
                    live_predicted_locals = []
                    if save_media:
                        # å¤„ç†æ™®é€šå›¾ç‰‡
                        image_urls = collect_image_urls(item)
                        if image_urls:
                            has_media = True
                            # åªæœ‰å½“åŒ…å«å¤šåª’ä½“æ–‡ä»¶æ—¶æ‰åˆ›å»ºæ–‡ä»¶å¤¹
                            item_dir = os.path.dirname(
                                get_output_path("dynamic", str(host_mid_int), str(id_str), "media")
                            )
                            os.makedirs(item_dir, exist_ok=True)
                            
                            # é¢„æµ‹æœ¬åœ°è·¯å¾„
                            for u in image_urls:
                                predicted_locals.append(os.path.relpath(predict_image_path(u, item_dir), base_output_dir))
                            
                            # ä¸‹è½½å¤šåª’ä½“æ–‡ä»¶
                            results = await download_images(image_urls, item_dir)
                            media_records = []
                            for media_url, local_path, ok in results:
                                if ok:
                                    rel_path = os.path.relpath(local_path, base_output_dir)
                                    media_records.append((media_url, rel_path, "image"))
                        
                        # å¤„ç†å®å†µåª’ä½“ï¼ˆliveå›¾ç‰‡+è§†é¢‘ï¼‰
                        live_media_pairs = collect_live_media_urls(item)
                        if live_media_pairs:
                            has_media = True
                            # åˆ›å»ºæ–‡ä»¶å¤¹ï¼ˆå¦‚æœè¿˜æœªåˆ›å»ºï¼‰
                            if not image_urls:
                                item_dir = os.path.dirname(
                                    get_output_path("dynamic", str(host_mid_int), str(id_str), "media")
                                )
                                os.makedirs(item_dir, exist_ok=True)
                            
                            live_results = await download_live_media(live_media_pairs, item_dir)
                            for image_url, video_url, image_path, video_path, ok in live_results:
                                if ok:
                                    # å°†å®å†µåª’ä½“è·¯å¾„åˆ†åˆ«è®°å½•
                                    image_rel = os.path.relpath(image_path, base_output_dir)
                                    video_rel = os.path.relpath(video_path, base_output_dir)
                                    live_predicted_locals.extend([image_rel, video_rel])
                        
                        # å¤„ç†è¡¨æƒ…
                        emoji_pairs = collect_emoji_urls(item)
                        if emoji_pairs:
                            has_media = True
                            # åˆ›å»ºæ–‡ä»¶å¤¹ï¼ˆå¦‚æœè¿˜æœªåˆ›å»ºï¼‰
                            if not image_urls and not live_media_pairs:
                                item_dir = os.path.dirname(
                                    get_output_path("dynamic", str(host_mid_int), str(id_str), "media")
                                )
                                os.makedirs(item_dir, exist_ok=True)
                            
                            emoji_results = await download_emojis(emoji_pairs, item_dir)
                            for emoji_url, emoji_path, ok in emoji_results:
                                if ok:
                                    # å°†è¡¨æƒ…è·¯å¾„è®°å½•åˆ°æ™®é€šåª’ä½“ä¸­
                                    emoji_rel = os.path.relpath(emoji_path, base_output_dir)
                                    predicted_locals.append(emoji_rel)

                    # ä¿å­˜å¤´åƒä¸€æ¬¡ï¼ˆè‹¥å­˜åœ¨ï¼‰
                    try:
                        face_url = None
                        modules_raw = item.get("modules")
                        if isinstance(modules_raw, dict):
                            face_url = modules_raw.get("module_author", {}).get("face")
                        elif isinstance(modules_raw, list):
                            for mod in modules_raw:
                                if isinstance(mod, dict) and mod.get("module_type") == "MODULE_TYPE_AUTHOR":
                                    face_url = mod.get("module_author", {}).get("user", {}).get("face") or mod.get("module_author", {}).get("face")
                                    if face_url:
                                        break
                        if not face_url:
                            face_url = item.get("user", {}).get("face")
                        if face_url and host_mid_int:
                            host_dir = os.path.dirname(get_output_path("dynamic", str(host_mid_int), "__host_meta.json"))
                            os.makedirs(host_dir, exist_ok=True)
                            exists = any(
                                name.lower().startswith("face.")
                                for name in os.listdir(host_dir)
                                if os.path.isfile(os.path.join(host_dir, name))
                            )
                            if not exists:
                                results = await download_images([face_url], host_dir)
                                for media_url, local_path, ok in results:
                                    if ok:
                                        _, ext = os.path.splitext(local_path)
                                        new_path = os.path.join(host_dir, f"face{ext}")
                                        try:
                                            if os.path.exists(new_path):
                                                os.remove(new_path)
                                        except Exception:
                                            pass
                                        try:
                                            os.replace(local_path, new_path)
                                        except Exception:
                                            pass
                                        break
                    except Exception as e:
                        logger.warning(f"ä¿å­˜å¤´åƒå¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{e}")

                    # è§„èŒƒåŒ–ä¿å­˜ + å›å†™é¢„æµ‹è·¯å¾„ï¼ˆé€—å·åˆ†éš”ï¼‰
                    try:
                        save_normalized_dynamic_item(conn, host_mid_int, item)
                        if predicted_locals or live_predicted_locals:
                            cursor = conn.cursor()
                            cursor.execute(
                                """
                                UPDATE dynamic_core SET
                                    media_locals = CASE
                                        WHEN media_locals IS NULL OR media_locals = '' THEN ?
                                        ELSE media_locals
                                    END,
                                    live_media_locals = CASE
                                        WHEN live_media_locals IS NULL OR live_media_locals = '' THEN ?
                                        ELSE live_media_locals
                                    END,
                                    live_media_count = ?
                                WHERE host_mid = ? AND id_str = ?
                                """,
                                (
                                    ",".join(predicted_locals) if predicted_locals else "",
                                    ",".join(live_predicted_locals) if live_predicted_locals else "",
                                    len(live_predicted_locals),
                                    str(host_mid_int),
                                    str(id_str),
                                ),
                            )
                            conn.commit()
                    except Exception as norm_err:
                        logger.warning(f"è§„èŒƒåŒ–ä¿å­˜å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰: {norm_err}")
                except Exception as perr:
                    logger.error(f"ä¿å­˜åŠ¨æ€è¯¦æƒ…å¤±è´¥ dynamic_id={dynamic_id}: {perr}")

            try:
                conn.close()
            except Exception:
                pass
        
        # ç›´æ¥è¿”å›Bç«™APIçš„åŸå§‹å“åº”æ•°æ®
        return data
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–åŠ¨æ€è¯¦æƒ…å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–åŠ¨æ€è¯¦æƒ…å¤±è´¥: {str(e)}")


@router.get("/types", summary="è·å–åŠ¨æ€ç±»å‹è¯´æ˜")
async def get_dynamic_types():
    """
    è·å–åŠ¨æ€ç±»å‹çš„è¯´æ˜ä¿¡æ¯
    """
    dynamic_types = {
        "DYNAMIC_TYPE_NONE": "æ— ç±»å‹",
        "DYNAMIC_TYPE_FORWARD": "è½¬å‘åŠ¨æ€",
        "DYNAMIC_TYPE_AV": "è§†é¢‘åŠ¨æ€",
        "DYNAMIC_TYPE_PGC": "ç•ªå‰§/å½±è§†åŠ¨æ€",
        "DYNAMIC_TYPE_COURSES": "è¯¾ç¨‹åŠ¨æ€",
        "DYNAMIC_TYPE_WORD": "æ–‡å­—åŠ¨æ€",
        "DYNAMIC_TYPE_DRAW": "å›¾ç‰‡åŠ¨æ€",
        "DYNAMIC_TYPE_ARTICLE": "æ–‡ç« åŠ¨æ€",
        "DYNAMIC_TYPE_MUSIC": "éŸ³é¢‘åŠ¨æ€",
        "DYNAMIC_TYPE_COMMON_SQUARE": "æ™®é€šæ–¹å½¢åŠ¨æ€",
        "DYNAMIC_TYPE_COMMON_VERTICAL": "æ™®é€šç«–ç‰ˆåŠ¨æ€",
        "DYNAMIC_TYPE_LIVE": "ç›´æ’­åŠ¨æ€",
        "DYNAMIC_TYPE_MEDIALIST": "æ”¶è—å¤¹åŠ¨æ€",
        "DYNAMIC_TYPE_COURSES_SEASON": "è¯¾ç¨‹åˆé›†åŠ¨æ€",
        "DYNAMIC_TYPE_COURSES_BATCH": "è¯¾ç¨‹æ‰¹æ¬¡åŠ¨æ€",
        "DYNAMIC_TYPE_AD": "å¹¿å‘ŠåŠ¨æ€",
        "DYNAMIC_TYPE_APPLET": "å°ç¨‹åºåŠ¨æ€",
        "DYNAMIC_TYPE_SUBSCRIPTION": "è®¢é˜…åŠ¨æ€",
        "DYNAMIC_TYPE_LIVE_RCMD": "ç›´æ’­æ¨èåŠ¨æ€",
        "DYNAMIC_TYPE_BANNER": "æ¨ªå¹…åŠ¨æ€",
        "DYNAMIC_TYPE_UGC_SEASON": "åˆé›†åŠ¨æ€",
        "DYNAMIC_TYPE_SUBSCRIPTION_NEW": "æ–°è®¢é˜…åŠ¨æ€"
    }
    
    return {"types": dynamic_types}